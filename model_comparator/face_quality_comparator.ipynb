{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed8a66db",
   "metadata": {},
   "source": [
    "# Сравниваем качество работы модели, оптимизированной разными способами"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee55252",
   "metadata": {},
   "source": [
    "## Подключение библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e4eb344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from facenet_pytorch import MTCNN\n",
    "import cv2\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import numpy as np\n",
    "import math\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from typing import Tuple\n",
    "import argparse\n",
    "import onnx\n",
    "import onnxruntime\n",
    "import sys\n",
    "import tensorrt as trt\n",
    "from PIL import Image\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "from typing import Tuple, Union\n",
    "import logging\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3751f413",
   "metadata": {},
   "source": [
    "## Используемые классы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc24596f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileFaceQualityNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = models.mobilenet_v3_small(pretrained=True)\n",
    "        # Заменяем последние 2 полносвязных слоя на имеющие нужную размерность\n",
    "        self.backbone.classifier[0] = nn.Linear(576, 256)\n",
    "        self.backbone.classifier[3] = nn.Linear(256, 1)\n",
    "\n",
    "        \n",
    "    def get_layers_names(self):\n",
    "         return dict(self.backbone.named_modules())\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83c93bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceQualityDataset(Dataset):\n",
    "    '''Класс для создания датасетов'''\n",
    "    \n",
    "    def __init__(self, dataframe: pd.DataFrame, device: str, transform: object, transfer_path: bool = False):\n",
    "        '''Входные параметры:\n",
    "        dataframe: pd.DataFrame - датафрейм с адресами изображений и скорами\n",
    "        device: str - имя устройства, на котором будут обрабатываться данные\n",
    "        transform: object - список преобразований, которым будут подвергнуты изображения и маски\n",
    "        transfer_path: bool - флаг, нужно ли передавать названия файлов через объект'''\n",
    "        \n",
    "        self.image_paths = dataframe['path']\n",
    "        self.target = dataframe['score']\n",
    "        self.transform = transform\n",
    "        self.data_len = len(dataframe.index)\n",
    "        self.device = device\n",
    "        self.transfer_path = transfer_path\n",
    "        \n",
    "    def __getitem__(self, index: int):\n",
    "        '''Входные параметры:\n",
    "        index: int - индекс для обращения к элементам датафрейма dataframe\n",
    "        Возвращаемые значения:\n",
    "        Tuple[torch.Tensor] - кортеж из тензорного представления изображения лица'''\n",
    "        \n",
    "        image = cv2.imread(self.image_paths[index])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype('float')/255.0\n",
    "\n",
    "        transformed = self.transform(image=image)\n",
    "        transformed_image = transformed['image'].to(self.device).float()\n",
    "        \n",
    "        target = torch.from_numpy(np.array(self.target[index])).to(self.device).float()\n",
    "        \n",
    "        if self.transfer_path:\n",
    "            return transformed_image, target, self.image_paths[index]\n",
    "        else:\n",
    "            return transformed_image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62c62a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelWrapper(nn.Module):\n",
    "    '''Класс, реализующий функционал для обучения нейросети классификации наличия маски на лице'''\n",
    "    \n",
    "    def __init__(self, model: object):\n",
    "        '''Конструктор класса\n",
    "        Входные параметры:\n",
    "        model: object - последовательность слоев или модель, через которую будут проходить данные'''\n",
    "        \n",
    "        super(ModelWrapper, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        '''Метод прямого прохода через объект класса\n",
    "        Входные параметры:\n",
    "        input_data: torch.Tensor - тензорное представление изображения лица\n",
    "        Возвращаемые значения: \n",
    "        output_data: torch.Tensor - предсказанные координаты ключевых точек в тензорном формате'''\n",
    "        \n",
    "        output_data = self.model(input_data)\n",
    "        return output_data\n",
    "    \n",
    "    \n",
    "    def valid(self, criterion: object, metric: object, valid_data_loader: DataLoader):\n",
    "        '''Метод для валидации модели\n",
    "        Входные параметры:\n",
    "        criterion: object - объект для вычисления loss\n",
    "        metric: object - объект для вычисления метрики качества\n",
    "        valid_data_loader: DataLoader - загрузчик данных для валидации\n",
    "        Возвращаемые значения:\n",
    "        result: dict - словарь со значениями loss и метрики при валидации'''\n",
    "        \n",
    "        self.model.eval()\n",
    "        valid_metrics = []\n",
    "        valid_losses = []\n",
    "        result = {}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, data in enumerate(valid_data_loader): \n",
    "                inputs = data[0]\n",
    "                target_scores = data[1]\n",
    "\n",
    "                pred_scores = self.model(inputs)\n",
    "                pred_scores = pred_scores.view(1, -1)[0]      # [16, 1] -> [16]\n",
    "                loss = criterion(pred_scores, target_scores)\n",
    "                metric_ = metric(pred_scores, target_scores)\n",
    "                valid_losses.append(loss.item())\n",
    "                valid_metrics.append(metric_.item())\n",
    "             \n",
    "        valid_loss = np.mean(valid_losses)\n",
    "        valid_metric = np.mean(valid_metrics)\n",
    "    \n",
    "        result['valid_loss'] = valid_loss\n",
    "        result['valid_metric'] = valid_metric\n",
    "\n",
    "        self.model.train()\n",
    "        \n",
    "        return result\n",
    "\n",
    "    \n",
    "    def load(self, path_to_model: str = './face_model.pth'):\n",
    "        '''Метод загрузки весов модели\n",
    "        Входные параметры:\n",
    "        path_to_model: str - директория с сохраненными весами модели'''\n",
    "        \n",
    "        self.model.load_state_dict(torch.load(path_to_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99136b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrtDynamicInfer():\n",
    "    '''Class for inference tensorrt models with a dynamic or static batch size'''\n",
    "    def __init__(self, path_to_model: str):\n",
    "        TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "        trt_runtime = trt.Runtime(TRT_LOGGER)\n",
    "        self.engine = load_engine(trt_runtime, path_to_model)\n",
    "        self.context =  self.engine.create_execution_context()\n",
    "\n",
    "\n",
    "    def inference(self, input: np.ndarray):\n",
    "        self.context.set_binding_shape(0, input.shape)\n",
    "        inputs, outputs, bindings, stream = TrtDynamicInfer.allocate_buffers(self.engine, current_batch_size=input.shape[0])\n",
    "        np.copyto(inputs[0].host, input.flatten())\n",
    "        output = do_inference(self.context, bindings=bindings, inputs=inputs, outputs=outputs, stream=stream)\n",
    "        return output\n",
    "\n",
    "\n",
    "    def inference_with_time(self, input: np.ndarray):\n",
    "        start_time = time.time()\n",
    "        self.context.set_binding_shape(0, input.shape)\n",
    "        inputs, outputs, bindings, stream = TrtDynamicInfer.allocate_buffers(self.engine, current_batch_size=input.shape[0])\n",
    "        np.copyto(inputs[0].host, input.flatten())\n",
    "        start_gpu_time = time.time()\n",
    "        output = do_inference(self.context, bindings=bindings, inputs=inputs, outputs=outputs, stream=stream)\n",
    "        end_gpu_time = time.time()\n",
    "        return output, start_time, start_gpu_time, end_gpu_time\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def allocate_buffers(engine, current_batch_size: int):\n",
    "        inputs = []\n",
    "        outputs = []\n",
    "        bindings = []\n",
    "        stream = cuda.Stream()\n",
    "        for binding in engine:\n",
    "            shape = engine.get_binding_shape(binding)\n",
    "            if shape[0] == -1:\n",
    "                # if the source trt model has a dynamic batch size (shape=(-1, C, H, W))\n",
    "                # we need to allocate memory with using batch size of received data\n",
    "                size = trt.volume(shape) * (-current_batch_size)\n",
    "            else:\n",
    "                # if the source trt model has a static batch size (shape=(B, C, H, W))\n",
    "                # we don't need to change anything\n",
    "                size = trt.volume(engine.get_binding_shape(binding))\n",
    "\n",
    "            dtype = trt.nptype(engine.get_binding_dtype(binding))\n",
    "            # Allocate host and device buffers\n",
    "            host_mem = cuda.pagelocked_empty(size, dtype)\n",
    "            device_mem = cuda.mem_alloc(host_mem.nbytes)\n",
    "            # Append the device buffer to device bindings.\n",
    "            bindings.append(int(device_mem))\n",
    "            # Append to the appropriate list.\n",
    "            if engine.binding_is_input(binding):\n",
    "                inputs.append(HostDeviceMem(host_mem, device_mem))\n",
    "            else:\n",
    "                outputs.append(HostDeviceMem(host_mem, device_mem))\n",
    "        return inputs, outputs, bindings, stream\n",
    "\n",
    "\n",
    "def do_inference(context, bindings, inputs, outputs, stream):\n",
    "    # Transfer input data to the GPU.\n",
    "    [cuda.memcpy_htod_async(inp.device, inp.host, stream) for inp in inputs]\n",
    "    flag = context.execute_async_v2(bindings=bindings, stream_handle=stream.handle)\n",
    "    # Transfer predictions back from the GPU.\n",
    "    [cuda.memcpy_dtoh_async(out.host, out.device, stream) for out in outputs]\n",
    "    # Synchronize the stream\n",
    "    stream.synchronize()\n",
    "    # Return only the host outputs.\n",
    "    return [out.host for out in outputs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d8472a",
   "metadata": {},
   "source": [
    "## Используемые функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "465b6c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb26e780",
   "metadata": {},
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ebec646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Команды генерации моделей (лучше выполнять через консоль из директории tensorrt_inference/src/)\n",
    "# python torch_to_onnx.py -n cr_fiqa -bt static -sh 8,3,112,112\n",
    "# python onnx_to_tensorrt.py -s cr_fiqa_static_batch_8x3x112x112.onnx -mbs 8 -bt static -prec fp16 -is 112\n",
    "# python onnx_to_tensorrt.py -s cr_fiqa_static_batch_8x3x112x112.onnx -mbs 8 -bt static -prec fp32 -is 112"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb4b0dda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3060'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58b77cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jul  1 11:53:34 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 470.129.06   Driver Version: 470.129.06   CUDA Version: 11.4     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |\r\n",
      "|  0%   49C    P2    36W / 170W |    544MiB / 12045MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      1144      G   /usr/lib/xorg/Xorg                 35MiB |\r\n",
      "|    0   N/A  N/A      2302      G   /usr/lib/xorg/Xorg                165MiB |\r\n",
      "|    0   N/A  N/A      2601      G   /usr/bin/gnome-shell               56MiB |\r\n",
      "|    0   N/A  N/A      4115      G   /usr/lib/firefox/firefox          149MiB |\r\n",
      "|    0   N/A  N/A     28368      G   ...AAAAAAAAA= --shared-files       22MiB |\r\n",
      "|    0   N/A  N/A     35159      G   telegram-desktop                    2MiB |\r\n",
      "|    0   N/A  N/A     35943      C   ...dima/anaconda3/bin/python       99MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4936b242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/dima/datasets/mt_dataset/1/336080.png</td>\n",
       "      <td>69.005943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/dima/datasets/mt_dataset/1/494830.png</td>\n",
       "      <td>77.437118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/dima/datasets/mt_dataset/1/398431.png</td>\n",
       "      <td>59.581174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/dima/datasets/mt_dataset/1/442370.png</td>\n",
       "      <td>41.054667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/dima/datasets/mt_dataset/1/124570.png</td>\n",
       "      <td>83.540477</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          path      score\n",
       "0  /home/dima/datasets/mt_dataset/1/336080.png  69.005943\n",
       "1  /home/dima/datasets/mt_dataset/1/494830.png  77.437118\n",
       "2  /home/dima/datasets/mt_dataset/1/398431.png  59.581174\n",
       "3  /home/dima/datasets/mt_dataset/1/442370.png  41.054667\n",
       "4  /home/dima/datasets/mt_dataset/1/124570.png  83.540477"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./dataset.csv')\n",
    "data.rename(columns={'target': 'score'}, inplace=True)\n",
    "data.rename(columns={'image_path': 'path'}, inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f67b39e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEGCAYAAABy53LJAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAY20lEQVR4nO3de5RdZ3nf8e/PMuIWEwEeEqFLJUB2onKzM9hKCi03syTbtXJrsRuWjSFVBTYhrBIQpSstK1mJuSQUB8daBgR2Qqw4CRe1KDEul7hJLSzZwTayEUxUgQcrsSirhuKC0fLTP84WHI/PzJwtzR6NZr6ftc46Z7+Xfd69l3Seed9373enqpAkaVgnHe8GSJJOLAYOSVIrBg5JUisGDklSKwYOSVIrJx/vBsyGU089tVatWnW8myFJJ5Tbbrvtm1U1MjF9QQSOVatWsWfPnuPdDEk6oST52qD0ToeqkqxPsi/JWJItA/KT5Mom/84kZ/blbUtyf5IvDaj3hma/e5O8q8tjkCQ9UmeBI8ki4CpgA7AWuCjJ2gnFNgBrmtcm4Oq+vI8A6wfs9yXARuC5VfVPgffMeOMlSZPqssdxFjBWVfur6iFgO70f/H4bgeuqZxewJMlSgKq6GfjWgP2+Driiqr7flLu/syOQJD1Kl4FjGXBv3/Z4k9a2zESnAS9K8oUkf53kBYMKJdmUZE+SPYcOHWrZdEnSZLoMHBmQNnFhrGHKTHQy8GRgHfAbwA1JHrWfqrqmqkaranRk5FEXBUiSjlKXgWMcWNG3vRy47yjKDNrvx5rhrVuBh4FTj7GtkqQhdRk4dgNrkqxOshi4ENgxocwO4OLm6qp1wANVdXCa/X4CeClAktOAxcA3Z7TlkqRJdRY4quowcDlwI3APcENV7U2yOcnmpthOYD8wBnwAeP2R+kmuB24BTk8ynuS1TdY24BnNZbrbgUvKteEladZkIfzmjo6OljcASlI7SW6rqtGJ6QviznFJmsqqLZ8amH7givNmuSUnBhc5lCS1YuCQJLVi4JAkteIch6QT0mTzEuDcRNcMHJIWjKmCjYbnUJUkqRV7HJKG4iWrOsLAIS1Q8zkQOCTVLYeqJEmt2OOQNKfZe5h77HFIkloxcEiSWnGoSlIn5vPk+0Jn4JDmuRNljuBEaacMHNIJx7/kdbw5xyFJaqXTHkeS9cD7gEXAB6vqign5afLPBR4EXl1Vtzd524Dzgfur6tkD9v1m4N3ASFX5zHGdkOZi78EhI02ns8CRZBFwFXAOMA7sTrKjqu7uK7YBWNO8zgaubt4BPgK8H7huwL5XNPv9elftl6S5GNjngi6Hqs4Cxqpqf1U9BGwHNk4osxG4rnp2AUuSLAWoqpuBb02y7/cCbwHm/wPTJWmO6XKoahlwb9/2OD/qTUxVZhlwcLKdJrkA+EZV3dEb6ZIEDjFp9nQZOAb9qk/sIQxT5keFkycAbwdeMe2XJ5uATQArV66crrgkaUhdDlWNAyv6tpcD9x1FmX7PBFYDdyQ50JS/PclPTixYVddU1WhVjY6MjBxF8yVJg3TZ49gNrEmyGvgGcCHwbyaU2QFcnmQ7vWGsB6pq0mGqqroLeNqR7SZ4jHpVlXT8OES28HTW46iqw8DlwI3APcANVbU3yeYkm5tiO4H9wBjwAeD1R+onuR64BTg9yXiS13bVVknS8Dq9j6OqdtILDv1pW/s+F3DZJHUvGmL/q46xidKc5F/xmstcckRD85p2SeCSI5KkluxxSJpV82EYbqH3vu1xSJJaMXBIkloxcEiSWnGOQ5pBC33sWwuDgUOaBfNhQlg6wsChOWOqH9eu/2K3pyANz8ChY9b2R9e/vqUTm5PjkqRWDBySpFYcqlJnZnJIyjkIae4wcEjSDFkof+AYOLSgtO0FLZQfAqkNA4d0FLwyTAuZgWMBmw9/Tc+HY5BONAYOPcp8+Gt6PhyDNFd1ejlukvVJ9iUZS7JlQH6SXNnk35nkzL68bUnuT/KlCXXeneTLTfmPJ1nS5TFIkh6ps8CRZBFwFbABWAtclGTthGIbgDXNaxNwdV/eR4D1A3Z9E/Dsqnou8BXgbTPbcknSVLocqjoLGKuq/QBJtgMbgbv7ymwErquqAnYlWZJkaVUdrKqbk6yauNOq+nTf5i7glzs7gnnCYRtJM6nLoaplwL192+NNWtsyU3kN8JeDMpJsSrInyZ5Dhw612KUkaSpdBo4MSKujKDN458nbgcPARwflV9U1VTVaVaMjIyPD7FKSNIQuh6rGgRV928uB+46izKMkuQQ4H3hZM8wlSZolXfY4dgNrkqxOshi4ENgxocwO4OLm6qp1wANVdXCqnSZZD7wVuKCqHuyi4ZKkyXUWOKrqMHA5cCNwD3BDVe1NsjnJ5qbYTmA/MAZ8AHj9kfpJrgduAU5PMp7ktU3W+4FTgJuSfDHJ1q6OQZL0aJ3eAFhVO+kFh/60rX2fC7hskroXTZL+rJlsoySpHZ/HIUlqxcAhSWrFwCFJasXAIUlqxcAhSWrFwCFJasXAIUlqxcAhSWrFJwDOIy6fLmk22OOQJLVi4JAktWLgkCS1YuCQJLVi4JAktWLgkCS1YuCQJLVi4JAkteINgCcgb/STdDx12uNIsj7JviRjSbYMyE+SK5v8O5Oc2Ze3Lcn9Sb40oc5TktyU5KvN+5O7PAZJ0iN1FjiSLAKuAjYAa4GLkqydUGwDsKZ5bQKu7sv7CLB+wK63AJ+pqjXAZ5ptSdIs6XKo6ixgrKr2AyTZDmwE7u4rsxG4rqoK2JVkSZKlVXWwqm5OsmrAfjcCL24+Xwt8HnhrN4dwfDkkJWkuGipwJDkf2FlVD7fY9zLg3r7tceDsIcosAw5Osd+fqKqDAFV1MMnTJmnzJnq9GFauXNmi2bPPACHpRDLsUNWFwFeTvCvJTw9ZJwPS6ijKHJWquqaqRqtqdGRkZCZ2KUliyMBRVa8CzgD+HvhwkluSbEpyyhTVxoEVfdvLgfuOosxE/5hkKUDzfv8QhyBJmiFDT45X1beBvwC2A0uBXwBuT/KGSarsBtYkWZ1kMb1ey44JZXYAFzdXV60DHjgyDDWFHcAlzedLgE8OewySpGM3VOBIckGSjwOfBR4DnFVVG4DnAW8eVKeqDgOXAzcC9wA3VNXeJJuTbG6K7QT2A2PAB4DX933n9cAtwOlJxpO8tsm6AjgnyVeBc5ptSdIsGfaqql8G3ltVN/cnVtWDSV4zWaWq2kkvOPSnbe37XMBlk9S9aJL0/w28bMh2S5Jm2LBDVQcnBo0k7wSoqs/MeKskSXPWsD2Oc3j0vRIbBqRJkiaY7JL7A1ecN8stmRlTBo4kr6M37/DMJHf2ZZ0C/G2XDZMkzU3T9Tj+BPhL4Hd55NIe36mqb3XWKknSnDVd4KiqOpDkURPYSZ5i8JCkhWeYHsf5wG307ujuv9O7gGd01C5J0hw1ZeCoqvOb99Wz0xxJ0lw33eT4mVPlV9XtM9scSdJcN91Q1e9NkVfAS2ewLZKkE8B0Q1Uvma2GSJJODNMNVb20qj6b5BcH5VfVx7ppliRprppuqOpf0FvY8F8OyCvAwCFJC8x0Q1X/qXm/dHaaI0ma64ZdVv2pSa5McnuS25K8L8lTu26cJGnuGXZ13O3AIeCX6C2xfgj4064aJUmau4ZdHfcpVfVbfdu/neTnO2iPJGmOG7bH8bkkFyY5qXn9a2DwOsGSpHltysCR5DtJvg38O3rrVj3UvLYDb5pu50nWJ9mXZCzJlgH5aeZOxpLc2X+n+mR1kzw/ya4kX0yyJ8lZwx+uJOlYTRk4quqUqnpS835SVZ3cvE6qqidNVTfJIuAqeg98WgtclGTthGIbgDXNaxNw9RB13wW8o6qeD/xmsy1JmiXDznGQ5Mn0fuAfdyRt4uNkJzgLGKuq/U397cBG4O6+MhuB65pnj+9KsiTJUmDVFHULOBK0fhy4b9hjOJ4mewKYJJ1ohgocSX4VeCOwHPgisA64hanXqloG3Nu3PQ6cPUSZZdPU/XXgxiTvoddj+rlJ2ryJXi+GlStXTtFMSVIbw06OvxF4AfC1Zv2qM+hdkjuVDEirIctMVfd1wJuqagW9eZYPDfryqrqmqkaranRkZGSapkqShjVs4PheVX0PIMljq+rLwOnT1BkHVvRtL+fRw0qTlZmq7iX8aKmTP6M3JCZJmiXDBo7xJEuATwA3Jfkk088t7AbWJFmdZDFwIbBjQpkdwMXN1VXrgAeq6uA0de+jt4YW9IbKvjrkMUiSZsBQcxxV9QvNx/+c5HP0JqX/apo6h5NcDtwILAK2VdXeJJub/K3ATuBcYAx4ELh0qrrNrv8t8L4kJwPfo5nHkCTNjjZXVZ0JvJDeXMPfVtVD09Wpqp30gkN/2ta+zwVcNmzdJv1vgJ8Ztt2SpJk17CKHvwlcCzwVOBX4cJL/2GXDJElz07A9jouAM/omyK8Abgd+u6uGSZLmpmEnxw/Qd+Mf8Fjg72e8NZKkOW+6R8f+Ab05je8De5Pc1GyfA/xN982TJM010w1V7WnebwM+3pf++U5aI0ma86Z7dOy1Rz4391Oc1mzuq6ofdNkwSdLcNOxaVS+md1XVAXrLgaxIcsk0ixxKkuahYa+q+j3gFVW1DyDJacD1eD+FJC04w15V9ZgjQQOgqr4CPKabJkmS5rJhexy3JfkQ8EfN9q/QmzCXJC0wwwaOzfSWBvk1enMcNwN/2FWjJElz17SBI8lJwG1V9Wzg97tvkiRpLpt2jqOqHgbuSOJj9CRJQw9VLaV35/itwHePJFbVBZ20SpI0Zw0bON7RaSskSSeM6daqehy9ifFnAXcBH6qqw7PRMEnS3DTdHMe1wCi9oLGB3o2AkqQFbLqhqrVV9RyA5j6OW7tvkiRpLpuux/HDhQyPZogqyfok+5KMJdkyID9Jrmzy72weTztt3SRvaPL2JnlX23ZJko7edD2O5yX5dvM5wOOb7dB7ZPiTJquYZBFwFb1nd4wDu5PsqKq7+4ptANY0r7OBq4Gzp6qb5CXARuC5VfX9JE9recySpGMw3bLqi45h32cBY1W1HyDJdno/+P2BYyNwXVUVsCvJkiRLgVVT1H0dcEVVfb9p4/3H0EZJUkvDLnJ4NJYB9/Ztjzdpw5SZqu5pwIuSfCHJXyd5waAvT7IpyZ4kew4dOnQMhyFJ6tdl4MiAtBqyzFR1TwaeDKwDfgO4IcmjylfVNVU1WlWjIyMjw7dakjSlYW8APBrjwIq+7eXAfUOWWTxF3XHgY83w1q1JHgZOBexWSNIs6LLHsRtYk2R189jZC4EdE8rsAC5urq5aBzxQVQenqfsJ4KXwwwdKLQa+2eFxSJL6dNbjqKrDSS4HbgQWAduqam+SzU3+VmAncC4wBjwIXDpV3WbX24BtSb4EPARc0vQ+JEmzoMuhKqpqJ73g0J+2te9z0XvOx1B1m/SHgFfNbEslScPqNHAsRKu2fOp4N0GSOtXlHIckaR4ycEiSWnGoSpKOk8mGtg9ccd4st6QdexySpFYMHJKkVgwckqRWDBySpFYMHJKkVgwckqRWDBySpFYMHJKkVgwckqRWDBySpFYMHJKkVgwckqRWDBySpFY6DRxJ1ifZl2QsyZYB+UlyZZN/Z5IzW9R9c5JKcmqXxyBJeqTOAkeSRcBVwAZgLXBRkrUTim0A1jSvTcDVw9RNsgI4B/h6V+2XJA3WZY/jLGCsqvY3zwnfDmycUGYjcF317AKWJFk6RN33Am8BqsP2S5IG6DJwLAPu7dseb9KGKTNp3SQXAN+oqjtmusGSpOl1+QTADEib2EOYrMzA9CRPAN4OvGLaL0820Rv+YuXKldMVlyQNqcsexziwom97OXDfkGUmS38msBq4I8mBJv32JD858cur6pqqGq2q0ZGRkWM8FEnSEV0Gjt3AmiSrkywGLgR2TCizA7i4ubpqHfBAVR2crG5V3VVVT6uqVVW1il6AObOq/qHD45Ak9elsqKqqDie5HLgRWARsq6q9STY3+VuBncC5wBjwIHDpVHW7aqskaXhdznFQVTvpBYf+tK19nwu4bNi6A8qsOvZWSpLa8M5xSVIrBg5JUisGDklSKwYOSVIrBg5JUisGDklSKwYOSVIrBg5JUisGDklSK53eOS5Jam/Vlk8NTD9wxXmz3JLB7HFIkloxcEiSWjFwSJJaMXBIkloxcEiSWjFwSJJaMXBIkloxcEiSWuk0cCRZn2RfkrEkWwbkJ8mVTf6dSc6crm6Sdyf5clP+40mWdHkMkqRH6ixwJFkEXAVsANYCFyVZO6HYBmBN89oEXD1E3ZuAZ1fVc4GvAG/r6hgkSY/WZY/jLGCsqvZX1UPAdmDjhDIbgeuqZxewJMnSqepW1aer6nBTfxewvMNjkCRN0OVaVcuAe/u2x4GzhyizbMi6AK8B/nTQlyfZRK8Xw8qVK9u0eyiTrSUjSfNdlz2ODEirIctMWzfJ24HDwEcHfXlVXVNVo1U1OjIyMkRzJUnD6LLHMQ6s6NteDtw3ZJnFU9VNcglwPvCyqpoYjCRJHeqyx7EbWJNkdZLFwIXAjglldgAXN1dXrQMeqKqDU9VNsh54K3BBVT3YYfslSQN01uOoqsNJLgduBBYB26pqb5LNTf5WYCdwLjAGPAhcOlXdZtfvBx4L3JQEYFdVbe7qOCRJj9Tpg5yqaie94NCftrXvcwGXDVu3SX/WDDdTktSCd45LkloxcEiSWjFwSJJaMXBIkloxcEiSWjFwSJJaMXBIkloxcEiSWjFwSJJaMXBIkloxcEiSWjFwSJJa6XSRQ0nSzJnsyaMHrjhvVtthj0OS1IqBQ5LUikNV05isayhJc8VUv1NdDGPZ45AktWLgkCS10mngSLI+yb4kY0m2DMhPkiub/DuTnDld3SRPSXJTkq8270/u8hgkSY/UWeBIsgi4CtgArAUuSrJ2QrENwJrmtQm4eoi6W4DPVNUa4DPNtiRplnTZ4zgLGKuq/VX1ELAd2DihzEbguurZBSxJsnSauhuBa5vP1wI/3+ExSJIm6PKqqmXAvX3b48DZQ5RZNk3dn6iqgwBVdTDJ0wZ9eZJN9HoxAP83yb6jOYh54lTgm8e7EXOU52Ywz8vkTqhzk3ceU/V/Miixy8CRAWk1ZJlh6k6pqq4BrmlTZ75KsqeqRo93O+Yiz81gnpfJeW66HaoaB1b0bS8H7huyzFR1/7EZzqJ5v38G2yxJmkaXgWM3sCbJ6iSLgQuBHRPK7AAubq6uWgc80AxDTVV3B3BJ8/kS4JMdHoMkaYLOhqqq6nCSy4EbgUXAtqram2Rzk78V2AmcC4wBDwKXTlW32fUVwA1JXgt8HfhXXR3DPOKQ3eQ8N4N5Xia34M9NqlpNHUiSFjjvHJcktWLgkCS1YuCYR5KsSPK5JPck2ZvkjU26y7Q0kixK8ndJ/luzveDPTZIlSf48yZebfzs/63npSfKm5v/Sl5Jcn+RxnhsDx3xzGPj3VfXTwDrgsmapFpdp+ZE3Avf0bXtu4H3AX1XVTwHPo3d+Fvx5SbIM+DVgtKqeTe9CnQvx3Bg45pOqOlhVtzefv0PvB2AZLtMCQJLlwHnAB/uSF/S5SfIk4J8DHwKoqoeq6v+wwM9Ln5OBxyc5GXgCvfvJFvy5MXDMU0lWAWcAX2DCMi3AwGVaFoD/ArwFeLgvbaGfm2cAh4APN0N4H0zyRDwvVNU3gPfQu+z/IL37zD6N58bAMR8l+THgL4Bfr6pvH+/2zAVJzgfur6rbjndb5piTgTOBq6vqDOC7LMChl0GauYuNwGrg6cATk7zq+LZqbjBwzDNJHkMvaHy0qj7WJLtMC/wz4IIkB+ittvzSJH+M52YcGK+qLzTbf04vkCz08wLwcuB/VdWhqvoB8DHg5/DcGDjmkyShN1Z9T1X9fl/Wgl+mpareVlXLq2oVvQnOz1bVq1jg56aq/gG4N8npTdLLgLtZ4Oel8XVgXZInNP+3XkZv3nDBnxvvHJ9HkrwQ+B/AXfxoHP8/0JvnuAFYSbNMS1V967g0cg5I8mLgzVV1fpKnssDPTZLn07tgYDGwn97SPyexwM8LQJJ3AK+kd8Xi3wG/CvwYC/zcGDgkSa04VCVJasXAIUlqxcAhSWrFwCFJasXAIUlqxcAhSWrFwCHNUc3CetKcY+CQZlCSJyb5VJI7mmc4vDLJC5L8zybt1iSnNM91+HCSu5rFBV/S1H91kj9L8l+BTzf725Zkd1Nu43E+RAn/opFm1nrgvqo6DyDJj9O74/iVVbW7Wcb8/9F7LghV9ZwkP0UvSJzW7ONngedW1beS/A695VFek2QJcGuS/15V353l45J+yB6HNLPuAl6e5J1JXkRvWYqDVbUboKq+XVWHgRcCf9SkfRn4GnAkcNzUt4TFK4AtSb4IfB54XLNP6bixxyHNoKr6SpKfAc4Ffhf4NDBoXZ9MsZv+3kSAX6qqfTPXSunY2OOQZlCSpwMPVtUf03sI0Drg6Ule0OSf0kx63wz8SpN2Gr1exKDgcCPwhmZ1VpKc0f1RSFOzxyHNrOcA707yMPAD4HX0eg1/kOTx9OY3Xg78IbA1yV30Vl59dVV9v4kP/X6L3pML72yCxwHg/Fk4DmlSro4rSWrFoSpJUisGDklSKwYOSVIrBg5JUisGDklSKwYOSVIrBg5JUiv/HyWxJyP3cOsnAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(data['score'].values, density=True, bins=50)\n",
    "plt.ylabel('Probability')\n",
    "plt.xlabel('score');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5d693665",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (112, 112)\n",
    "batch_size = 8\n",
    "device = 'cuda'\n",
    "\n",
    "criterion  = nn.MSELoss()\n",
    "metric = nn.L1Loss()\n",
    "\n",
    "transform = A.Compose([\n",
    "    A.Resize(*image_size),\n",
    "    ToTensorV2()])\n",
    "\n",
    "dataset = FaceQualityDataset(data, device, transform)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428177aa",
   "metadata": {},
   "source": [
    "## Тестирование pytorch (cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "10847e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion  = nn.MSELoss()\n",
    "metric = nn.L1Loss()\n",
    "\n",
    "# Важно в eval переключить, иначе ошибка на валидации возрастет\n",
    "model = MobileFaceQualityNet().eval().to(device)\n",
    "model_wrapper = ModelWrapper(model=model)\n",
    "model_wrapper.load(path_to_model = '../models/pytorch/face_quality_cr-fiqa_mt_mae_3,6_ep_3.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "15409329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 15.26\n",
      "Metric: 2.983\n"
     ]
    }
   ],
   "source": [
    "result = model_wrapper.valid(criterion, metric, data_loader)\n",
    "print(f'Loss: {result[\"valid_loss\"]:.4}', end='\\n')\n",
    "print(f'Metric: {result[\"valid_metric\"]:.4}', end='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1f5e2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_pytorch(model: object, dataloader: object) -> np.ndarray:\n",
    "    model.eval()\n",
    "    result = np.array([])\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in enumerate(dataloader): \n",
    "            inputs = data[0]\n",
    "            pred_scores = model(inputs)\n",
    "            pred_scores = pred_scores.view(1, -1)[0]      # [16, 1] -> [16]\n",
    "            pred_scores_np = pred_scores.detach().cpu().numpy()\n",
    "            result = np.hstack([result, pred_scores_np])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3c995b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_y = get_pred_pytorch(model, data_loader)\n",
    "data['dist_cr_fiq'] = valid_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97458a4d",
   "metadata": {},
   "source": [
    "## Тестирование ONNX (CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cf78a92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnxmodel_path = '../models/onnx/cr_fiqa_dynamic_batch_1x3x112x112.onnx'\n",
    "onnx_model = onnx.load(onnxmodel_path)\n",
    "onnx.checker.check_model(onnx_model)\n",
    "ort_session = onnxruntime.InferenceSession(onnxmodel_path, providers=['CPUExecutionProvider'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "009cec8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_onnx(ort_session: object, dataloader: object) -> np.ndarray:\n",
    "    result = np.array([])\n",
    "    for batch_idx, data in enumerate(dataloader): \n",
    "        inputs = data[0]\n",
    "        ort_outs = ort_session.run(None, {ort_session.get_inputs()[0].name: to_numpy(inputs)})\n",
    "        ort_outs = ort_outs[0].reshape(ort_outs[0].shape[0],)\n",
    "        result = np.hstack([result, ort_outs])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cdc34045",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_y_onnx = get_pred_onnx(ort_session, data_loader)\n",
    "data['onnx_cpu'] = valid_y_onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff335671",
   "metadata": {},
   "source": [
    "## Тестирование TENSORRT 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "bc69455f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07/01/2022-12:26:43] [TRT] [W] TensorRT was linked against cuBLAS/cuBLAS LT 11.8.0 but loaded cuBLAS/cuBLAS LT 11.2.1\n",
      "[07/01/2022-12:26:43] [TRT] [W] TensorRT was linked against cuBLAS/cuBLAS LT 11.8.0 but loaded cuBLAS/cuBLAS LT 11.2.1\n"
     ]
    }
   ],
   "source": [
    "path_to_model = '../models/trt/cr_fiqa_1-8-16x3x112x112_fp16.trt'\n",
    "trt_infer = TrtDynamicInfer(path_to_model=path_to_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "70762bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_tensorrt(trt_infer: object, dataloader: object) -> np.ndarray:\n",
    "    result = np.array([])\n",
    "    for batch_idx, data in enumerate(dataloader): \n",
    "        inputs = data[0]\n",
    "        trt_output = trt_infer.inference(to_numpy(inputs))\n",
    "        result = np.hstack([result, trt_output[0]])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1236ccfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trt_16 = get_pred_tensorrt(trt_infer, data_loader)\n",
    "data['trt_16'] = y_trt_16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ab8c79",
   "metadata": {},
   "source": [
    "## Тестирование TENSORRT 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f05c51e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07/01/2022-12:27:42] [TRT] [W] TensorRT was linked against cuBLAS/cuBLAS LT 11.8.0 but loaded cuBLAS/cuBLAS LT 11.2.1\n",
      "[07/01/2022-12:27:42] [TRT] [W] TensorRT was linked against cuBLAS/cuBLAS LT 11.8.0 but loaded cuBLAS/cuBLAS LT 11.2.1\n"
     ]
    }
   ],
   "source": [
    "path_to_model = '../models/trt/cr_fiqa_1-8-16x3x112x112_fp32.trt'\n",
    "trt_infer = TrtDynamicInfer(path_to_model=path_to_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "040e97b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trt_32 = get_pred_tensorrt(trt_infer, data_loader)\n",
    "data['trt_32'] = y_trt_32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cec06fe",
   "metadata": {},
   "source": [
    "## Сравнение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0e249edb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>score</th>\n",
       "      <th>dist_cr_fiq</th>\n",
       "      <th>onnx_cpu</th>\n",
       "      <th>trt_16</th>\n",
       "      <th>trt_32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/dima/datasets/mt_dataset/1/336080.png</td>\n",
       "      <td>69.005943</td>\n",
       "      <td>71.733521</td>\n",
       "      <td>71.734154</td>\n",
       "      <td>71.875000</td>\n",
       "      <td>71.733917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/dima/datasets/mt_dataset/1/494830.png</td>\n",
       "      <td>77.437118</td>\n",
       "      <td>78.443466</td>\n",
       "      <td>78.443504</td>\n",
       "      <td>78.375000</td>\n",
       "      <td>78.443687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/dima/datasets/mt_dataset/1/398431.png</td>\n",
       "      <td>59.581174</td>\n",
       "      <td>54.542908</td>\n",
       "      <td>54.541039</td>\n",
       "      <td>54.531250</td>\n",
       "      <td>54.541832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/dima/datasets/mt_dataset/1/442370.png</td>\n",
       "      <td>41.054667</td>\n",
       "      <td>40.565643</td>\n",
       "      <td>40.567150</td>\n",
       "      <td>40.500000</td>\n",
       "      <td>40.565804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/dima/datasets/mt_dataset/1/124570.png</td>\n",
       "      <td>83.540477</td>\n",
       "      <td>80.295120</td>\n",
       "      <td>80.297531</td>\n",
       "      <td>80.250000</td>\n",
       "      <td>80.293701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41874</th>\n",
       "      <td>/home/dima/datasets/mt_dataset/0/561290.png</td>\n",
       "      <td>16.048279</td>\n",
       "      <td>19.787758</td>\n",
       "      <td>19.788050</td>\n",
       "      <td>19.781250</td>\n",
       "      <td>19.787819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41875</th>\n",
       "      <td>/home/dima/datasets/mt_dataset/0/534580.png</td>\n",
       "      <td>11.017109</td>\n",
       "      <td>11.660115</td>\n",
       "      <td>11.659534</td>\n",
       "      <td>11.609375</td>\n",
       "      <td>11.660245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41876</th>\n",
       "      <td>/home/dima/datasets/mt_dataset/0/530660.png</td>\n",
       "      <td>40.944187</td>\n",
       "      <td>39.605728</td>\n",
       "      <td>39.603436</td>\n",
       "      <td>39.531250</td>\n",
       "      <td>39.604469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41877</th>\n",
       "      <td>/home/dima/datasets/mt_dataset/0/53480.png</td>\n",
       "      <td>13.633580</td>\n",
       "      <td>15.934447</td>\n",
       "      <td>15.934464</td>\n",
       "      <td>15.984375</td>\n",
       "      <td>15.936285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41878</th>\n",
       "      <td>/home/dima/datasets/mt_dataset/0/158310.png</td>\n",
       "      <td>15.289119</td>\n",
       "      <td>12.130036</td>\n",
       "      <td>12.129135</td>\n",
       "      <td>12.140625</td>\n",
       "      <td>12.129657</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41879 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              path      score  dist_cr_fiq  \\\n",
       "0      /home/dima/datasets/mt_dataset/1/336080.png  69.005943    71.733521   \n",
       "1      /home/dima/datasets/mt_dataset/1/494830.png  77.437118    78.443466   \n",
       "2      /home/dima/datasets/mt_dataset/1/398431.png  59.581174    54.542908   \n",
       "3      /home/dima/datasets/mt_dataset/1/442370.png  41.054667    40.565643   \n",
       "4      /home/dima/datasets/mt_dataset/1/124570.png  83.540477    80.295120   \n",
       "...                                            ...        ...          ...   \n",
       "41874  /home/dima/datasets/mt_dataset/0/561290.png  16.048279    19.787758   \n",
       "41875  /home/dima/datasets/mt_dataset/0/534580.png  11.017109    11.660115   \n",
       "41876  /home/dima/datasets/mt_dataset/0/530660.png  40.944187    39.605728   \n",
       "41877   /home/dima/datasets/mt_dataset/0/53480.png  13.633580    15.934447   \n",
       "41878  /home/dima/datasets/mt_dataset/0/158310.png  15.289119    12.130036   \n",
       "\n",
       "        onnx_cpu     trt_16     trt_32  \n",
       "0      71.734154  71.875000  71.733917  \n",
       "1      78.443504  78.375000  78.443687  \n",
       "2      54.541039  54.531250  54.541832  \n",
       "3      40.567150  40.500000  40.565804  \n",
       "4      80.297531  80.250000  80.293701  \n",
       "...          ...        ...        ...  \n",
       "41874  19.788050  19.781250  19.787819  \n",
       "41875  11.659534  11.609375  11.660245  \n",
       "41876  39.603436  39.531250  39.604469  \n",
       "41877  15.934464  15.984375  15.936285  \n",
       "41878  12.129135  12.140625  12.129657  \n",
       "\n",
       "[41879 rows x 6 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780b38cc",
   "metadata": {},
   "source": [
    "### GT vs pytorch_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "233c2ebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.255310180743914"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse = criterion(torch.tensor(data['score'].values), torch.tensor(data['dist_cr_fiq'].values))\n",
    "mse.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a1015699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.9827817569422703"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae = metric(torch.tensor(data['score'].values), torch.tensor(data['dist_cr_fiq'].values))\n",
    "mae.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c0f422",
   "metadata": {},
   "source": [
    "### GT vs onnx_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0cb66c34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.255218025258749"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse = criterion(torch.tensor(data['score'].values), torch.tensor(data['onnx_cpu'].values))\n",
    "mse.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "26dabe19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.982760017611324"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae = metric(torch.tensor(data['score'].values), torch.tensor(data['onnx_cpu'].values))\n",
    "mae.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3a8485",
   "metadata": {},
   "source": [
    "### GT vs pytorch_trt_16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "31e0f4e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.284098873406807"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse = criterion(torch.tensor(data['score'].values), torch.tensor(data['trt_16'].values))\n",
    "mse.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e4de2d77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.9857487399433262"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae = metric(torch.tensor(data['score'].values), torch.tensor(data['trt_16'].values))\n",
    "mae.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a7b6dd",
   "metadata": {},
   "source": [
    "### GT vs pytorch_trt_32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d2f2c17d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.255265750922847"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse = criterion(torch.tensor(data['score'].values), torch.tensor(data['trt_32'].values))\n",
    "mse.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9361ab10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.982771172137754"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae = metric(torch.tensor(data['score'].values), torch.tensor(data['trt_32'].values))\n",
    "mae.item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
