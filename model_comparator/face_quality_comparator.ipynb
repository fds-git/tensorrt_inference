{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed8a66db",
   "metadata": {},
   "source": [
    "# Сравниваем качество работы модели, оптимизированной разными способами"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f039c16e",
   "metadata": {},
   "source": [
    "Провести эксперименты по вычислению mse, mae с использованием разных размеров батчей, так как не сходятся ошибки из блокнота с обученияем (батчи) и в этом блокноте (единоразовое вычисление)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee55252",
   "metadata": {},
   "source": [
    "## Подключение библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9e4eb344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from facenet_pytorch import MTCNN\n",
    "import cv2\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import numpy as np\n",
    "import math\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from typing import Tuple\n",
    "import argparse\n",
    "import onnx\n",
    "import onnxruntime\n",
    "import sys\n",
    "import tensorrt as trt\n",
    "from PIL import Image\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "from typing import Tuple, Union\n",
    "import logging\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3751f413",
   "metadata": {},
   "source": [
    "## Используемые классы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dc24596f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileFaceQualityNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = models.mobilenet_v3_small(pretrained=True)\n",
    "        # Заменяем последние 2 полносвязных слоя на имеющие нужную размерность\n",
    "        self.backbone.classifier[0] = nn.Linear(576, 256)\n",
    "        self.backbone.classifier[3] = nn.Linear(256, 1)\n",
    "\n",
    "        \n",
    "    def get_layers_names(self):\n",
    "         return dict(self.backbone.named_modules())\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "83c93bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceQualityDataset(Dataset):\n",
    "    '''Класс для создания датасетов'''\n",
    "    \n",
    "    def __init__(self, dataframe: pd.DataFrame, device: str, transform: object, transfer_path: bool = False):\n",
    "        '''Входные параметры:\n",
    "        dataframe: pd.DataFrame - датафрейм с адресами изображений и скорами\n",
    "        device: str - имя устройства, на котором будут обрабатываться данные\n",
    "        transform: object - список преобразований, которым будут подвергнуты изображения и маски\n",
    "        transfer_path: bool - флаг, нужно ли передавать названия файлов через объект'''\n",
    "        \n",
    "        self.image_paths = dataframe['path']\n",
    "        self.target = dataframe['score']\n",
    "        self.transform = transform\n",
    "        self.data_len = len(dataframe.index)\n",
    "        self.device = device\n",
    "        self.transfer_path = transfer_path\n",
    "        \n",
    "    def __getitem__(self, index: int):\n",
    "        '''Входные параметры:\n",
    "        index: int - индекс для обращения к элементам датафрейма dataframe\n",
    "        Возвращаемые значения:\n",
    "        Tuple[torch.Tensor] - кортеж из тензорного представления изображения лица'''\n",
    "        \n",
    "        image = cv2.imread(self.image_paths[index])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype('float')/255.0\n",
    "\n",
    "        transformed = self.transform(image=image)\n",
    "        transformed_image = transformed['image'].to(self.device).float()\n",
    "        \n",
    "        target = torch.from_numpy(np.array(self.target[index])).to(self.device).float()\n",
    "        \n",
    "        if self.transfer_path:\n",
    "            return transformed_image, target, self.image_paths[index]\n",
    "        else:\n",
    "            return transformed_image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "62c62a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelWrapper(nn.Module):\n",
    "    '''Класс, реализующий функционал для обучения нейросети классификации наличия маски на лице'''\n",
    "    \n",
    "    def __init__(self, model: object):\n",
    "        '''Конструктор класса\n",
    "        Входные параметры:\n",
    "        model: object - последовательность слоев или модель, через которую будут проходить данные'''\n",
    "        \n",
    "        super(ModelWrapper, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        '''Метод прямого прохода через объект класса\n",
    "        Входные параметры:\n",
    "        input_data: torch.Tensor - тензорное представление изображения лица\n",
    "        Возвращаемые значения: \n",
    "        output_data: torch.Tensor - предсказанные координаты ключевых точек в тензорном формате'''\n",
    "        \n",
    "        output_data = self.model(input_data)\n",
    "        return output_data\n",
    "    \n",
    "    \n",
    "    def valid(self, criterion: object, metric: object, valid_data_loader: DataLoader):\n",
    "        '''Метод для валидации модели\n",
    "        Входные параметры:\n",
    "        criterion: object - объект для вычисления loss\n",
    "        metric: object - объект для вычисления метрики качества\n",
    "        valid_data_loader: DataLoader - загрузчик данных для валидации\n",
    "        Возвращаемые значения:\n",
    "        result: dict - словарь со значениями loss и метрики при валидации'''\n",
    "        \n",
    "        self.model.eval()\n",
    "        valid_metrics = []\n",
    "        valid_losses = []\n",
    "        result = {}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, data in enumerate(valid_data_loader): \n",
    "                inputs = data[0]\n",
    "                target_scores = data[1]\n",
    "\n",
    "                pred_scores = self.model(inputs)\n",
    "                pred_scores = pred_scores.view(1, -1)[0]      # [16, 1] -> [16]\n",
    "                loss = criterion(pred_scores, target_scores)\n",
    "                metric_ = metric(pred_scores, target_scores)\n",
    "                valid_losses.append(loss.item())\n",
    "                valid_metrics.append(metric_.item())\n",
    "             \n",
    "        valid_loss = np.mean(valid_losses)\n",
    "        valid_metric = np.mean(valid_metrics)\n",
    "    \n",
    "        result['valid_loss'] = valid_loss\n",
    "        result['valid_metric'] = valid_metric\n",
    "\n",
    "        self.model.train()\n",
    "        \n",
    "        return result\n",
    "\n",
    "    \n",
    "    def load(self, path_to_model: str = './face_model.pth'):\n",
    "        '''Метод загрузки весов модели\n",
    "        Входные параметры:\n",
    "        path_to_model: str - директория с сохраненными весами модели'''\n",
    "        \n",
    "        self.model.load_state_dict(torch.load(path_to_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "7ba8a840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://github.com/NVIDIA/object-detection-tensorrt-example/blob/master/SSD_Model/utils/inference.py\n",
    "\n",
    "class TrtInfer():\n",
    "    def __init__(self, path_to_model: str):\n",
    "        TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "        trt_runtime = trt.Runtime(TRT_LOGGER)\n",
    "        self.engine = load_engine(trt_runtime, path_to_model)\n",
    "        self.inputs, self.outputs, self.bindings, self.stream = allocate_buffers(self.engine)\n",
    "        self.context =  self.engine.create_execution_context()\n",
    "\n",
    "    def inference(self, input: np.ndarray):\n",
    "        np.copyto(self.inputs[0].host, input.flatten())\n",
    "        output = do_inference(self.context, bindings=self.bindings, inputs=self.inputs, \n",
    "                              outputs=self.outputs, stream=self.stream)\n",
    "        return output\n",
    "    \n",
    "\n",
    "def save_engine(engine, engine_dest_path):\n",
    "    print('Engine:', engine)\n",
    "    buf = engine.serialize()\n",
    "    with open(engine_dest_path, 'wb') as f:\n",
    "        f.write(buf)\n",
    "\n",
    "\n",
    "def load_engine(trt_runtime, engine_path):\n",
    "    with open(engine_path, 'rb') as f:\n",
    "        engine_data = f.read()\n",
    "    engine = trt_runtime.deserialize_cuda_engine(engine_data)\n",
    "    return engine\n",
    "\n",
    "\n",
    "# Simple helper data class that's a little nicer to use than a 2-tuple.\n",
    "class HostDeviceMem(object):\n",
    "    def __init__(self, host_mem, device_mem):\n",
    "        self.host = host_mem\n",
    "        self.device = device_mem\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Host:\\n\" + str(self.host) + \"\\nDevice:\\n\" + str(self.device)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "\n",
    "# Allocates all buffers required for an engine, i.e. host/device inputs/outputs.\n",
    "def allocate_buffers(engine):\n",
    "    \"\"\"Allocates host and device buffer for TRT engine inference.\n",
    "    Args:\n",
    "        engine (trt.ICudaEngine): TensorRT engine\n",
    "    Returns:\n",
    "        inputs [HostDeviceMem]: engine input memory\n",
    "        outputs [HostDeviceMem]: engine output memory\n",
    "        bindings [int]: buffer to device bindings\n",
    "        stream (cuda.Stream): cuda stream for engine inference synchronization\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    bindings = []\n",
    "    stream = cuda.Stream()\n",
    "    for binding in engine:\n",
    "        #print(f\"engine max batch size {engine.max_batch_size}\")\n",
    "        #print(f\"engine binding shape {engine.get_binding_shape(binding)}\")\n",
    "        size = trt.volume(engine.get_binding_shape(binding)) #* engine.max_batch_size\n",
    "        dtype = trt.nptype(engine.get_binding_dtype(binding))\n",
    "        # Allocate host and device buffers\n",
    "        host_mem = cuda.pagelocked_empty(size, dtype)\n",
    "        device_mem = cuda.mem_alloc(host_mem.nbytes)\n",
    "        # Append the device buffer to device bindings.\n",
    "        bindings.append(int(device_mem))\n",
    "        # Append to the appropriate list.\n",
    "        if engine.binding_is_input(binding):\n",
    "            inputs.append(HostDeviceMem(host_mem, device_mem))\n",
    "        else:\n",
    "            outputs.append(HostDeviceMem(host_mem, device_mem))\n",
    "    return inputs, outputs, bindings, stream\n",
    "\n",
    "\n",
    "# This function is generalized for multiple inputs/outputs.\n",
    "# inputs and outputs are expected to be lists of HostDeviceMem objects.\n",
    "def do_inference(context, bindings, inputs, outputs, stream, batch_size=1):\n",
    "    # Transfer input data to the GPU.\n",
    "    [cuda.memcpy_htod_async(inp.device, inp.host, stream) for inp in inputs]\n",
    "    # Run inference.\n",
    "    context.execute_async(batch_size=batch_size, bindings=bindings, stream_handle=stream.handle)\n",
    "    # Transfer predictions back from the GPU.\n",
    "    [cuda.memcpy_dtoh_async(out.host, out.device, stream) for out in outputs]\n",
    "    # Synchronize the stream\n",
    "    stream.synchronize()\n",
    "    # Return only the host outputs.\n",
    "    return [out.host for out in outputs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d8472a",
   "metadata": {},
   "source": [
    "## Используемые функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "465b6c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb26e780",
   "metadata": {},
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebec646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Команды генерации моделей (лучше выполнять через консоль из директории tensorrt_inference/src/)\n",
    "# python torch_to_onnx.py -n cr_fiqa -bt static -sh 8,3,112,112\n",
    "# python onnx_to_tensorrt.py -s cr_fiqa_static_batch_8x3x112x112.onnx -mbs 8 -bt static -prec fp16 -is 112\n",
    "# python onnx_to_tensorrt.py -s cr_fiqa_static_batch_8x3x112x112.onnx -mbs 8 -bt static -prec fp32 -is 112"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fb4b0dda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3060'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "58b77cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jun 27 18:04:37 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 470.129.06   Driver Version: 470.129.06   CUDA Version: 11.4     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |\r\n",
      "|  0%   45C    P8    10W / 170W |   3360MiB / 12045MiB |      6%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      1135      G   /usr/lib/xorg/Xorg                 35MiB |\r\n",
      "|    0   N/A  N/A      1948      G   /usr/lib/xorg/Xorg                203MiB |\r\n",
      "|    0   N/A  N/A      2077      G   /usr/bin/gnome-shell               31MiB |\r\n",
      "|    0   N/A  N/A      5351      G   /usr/lib/firefox/firefox          142MiB |\r\n",
      "|    0   N/A  N/A      9256      G   telegram-desktop                    2MiB |\r\n",
      "|    0   N/A  N/A     11700      G   ...AAAAAAAAA= --shared-files       37MiB |\r\n",
      "|    0   N/A  N/A     12119      C   ...dima/anaconda3/bin/python     2893MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4936b242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/dima/datasets/metro_dataset/1/336080.png</td>\n",
       "      <td>69.005943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/dima/datasets/metro_dataset/1/494830.png</td>\n",
       "      <td>77.437118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/dima/datasets/metro_dataset/1/398431.png</td>\n",
       "      <td>59.581174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/dima/datasets/metro_dataset/1/442370.png</td>\n",
       "      <td>41.054667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/dima/datasets/metro_dataset/1/124570.png</td>\n",
       "      <td>83.540477</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             path      score\n",
       "0  /home/dima/datasets/metro_dataset/1/336080.png  69.005943\n",
       "1  /home/dima/datasets/metro_dataset/1/494830.png  77.437118\n",
       "2  /home/dima/datasets/metro_dataset/1/398431.png  59.581174\n",
       "3  /home/dima/datasets/metro_dataset/1/442370.png  41.054667\n",
       "4  /home/dima/datasets/metro_dataset/1/124570.png  83.540477"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./metro_data.csv')\n",
    "data.rename(columns={'target': 'score'}, inplace=True)\n",
    "data.rename(columns={'image_path': 'path'}, inplace=True)\n",
    "data = data.iloc[:41872] # Это чтобы кратно 8 было, иначе ошибка может быть у статичных моделей\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f67b39e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(data['score'].values, density=True, bins=50)\n",
    "plt.ylabel('Probability')\n",
    "plt.xlabel('score');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5d693665",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (112, 112)\n",
    "batch_size = 8\n",
    "device = 'cuda'\n",
    "\n",
    "criterion  = nn.MSELoss()\n",
    "metric = nn.L1Loss()\n",
    "\n",
    "transform = A.Compose([\n",
    "    A.Resize(*image_size),\n",
    "    ToTensorV2()])\n",
    "\n",
    "dataset = FaceQualityDataset(data, device, transform)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428177aa",
   "metadata": {},
   "source": [
    "## Тестирование pytorch (cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "10847e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion  = nn.MSELoss()\n",
    "metric = nn.L1Loss()\n",
    "\n",
    "# Важно в eval переключить, иначе ошибка на валидации возрастет\n",
    "model = MobileFaceQualityNet().eval().to(device)\n",
    "model_wrapper = ModelWrapper(model=model)\n",
    "model_wrapper.load(path_to_model = '../models/pytorch/face_quality_cr-fiqa_metro_mae_3,6_ep_3.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "15409329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 15.26\n",
      "Metric: 2.983\n"
     ]
    }
   ],
   "source": [
    "result = model_wrapper.valid(criterion, metric, data_loader)\n",
    "print(f'Loss: {result[\"valid_loss\"]:.4}', end='\\n')\n",
    "print(f'Metric: {result[\"valid_metric\"]:.4}', end='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f1f5e2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_pytorch(model: object, dataloader: object) -> np.ndarray:\n",
    "    model.eval()\n",
    "    result = np.array([])\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in enumerate(dataloader): \n",
    "            inputs = data[0]\n",
    "            pred_scores = model(inputs)\n",
    "            pred_scores = pred_scores.view(1, -1)[0]      # [16, 1] -> [16]\n",
    "            pred_scores_np = pred_scores.detach().cpu().numpy()\n",
    "            result = np.hstack([result, pred_scores_np])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3c995b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_y = get_pred_pytorch(model, data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8ca0d6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['dist_cr_fiq'] = valid_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97458a4d",
   "metadata": {},
   "source": [
    "## Тестирование ONNX (CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cf78a92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnxmodel_path = '../models/onnx/cr_fiqa_static_batch_8x3x112x112.onnx'\n",
    "onnx_model = onnx.load(onnxmodel_path)\n",
    "onnx.checker.check_model(onnx_model)\n",
    "ort_session = onnxruntime.InferenceSession(onnxmodel_path, providers=['CPUExecutionProvider'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "009cec8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_onnx(ort_session: object, dataloader: object) -> np.ndarray:\n",
    "    result = np.array([])\n",
    "    for batch_idx, data in enumerate(dataloader): \n",
    "        inputs = data[0]\n",
    "        ort_outs = ort_session.run(None, {ort_session.get_inputs()[0].name: to_numpy(inputs)})\n",
    "        ort_outs = ort_outs[0].reshape(ort_outs[0].shape[0],)\n",
    "        result = np.hstack([result, ort_outs])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cdc34045",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_y_onnx = get_pred_onnx(ort_session, data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e10c411c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['onnx_cpu'] = valid_y_onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff335671",
   "metadata": {},
   "source": [
    "## Тестирование TENSORRT 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "bc69455f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06/27/2022-17:54:33] [TRT] [W] TensorRT was linked against cuBLAS/cuBLAS LT 11.8.0 but loaded cuBLAS/cuBLAS LT 11.2.1\n",
      "[06/27/2022-17:54:33] [TRT] [W] TensorRT was linked against cuBLAS/cuBLAS LT 11.8.0 but loaded cuBLAS/cuBLAS LT 11.2.1\n"
     ]
    }
   ],
   "source": [
    "path_to_model = '../models/trt/model_static_max_batch_8_112_fp16.trt'\n",
    "trt_infer = TrtInfer(path_to_model=path_to_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "70762bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_tensorrt(trt_infer: object, dataloader: object) -> np.ndarray:\n",
    "    result = np.array([])\n",
    "    for batch_idx, data in enumerate(dataloader): \n",
    "        inputs = data[0]\n",
    "        trt_output = trt_infer.inference(to_numpy(inputs))\n",
    "        result = np.hstack([result, trt_output[0]])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1236ccfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trt_16 = get_pred_tensorrt(trt_infer, data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a71f479d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['trt_16'] = y_trt_16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ab8c79",
   "metadata": {},
   "source": [
    "## Тестирование TENSORRT 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f05c51e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06/27/2022-17:46:57] [TRT] [W] TensorRT was linked against cuBLAS/cuBLAS LT 11.8.0 but loaded cuBLAS/cuBLAS LT 11.2.1\n",
      "[06/27/2022-17:46:57] [TRT] [W] TensorRT was linked against cuBLAS/cuBLAS LT 11.8.0 but loaded cuBLAS/cuBLAS LT 11.2.1\n"
     ]
    }
   ],
   "source": [
    "path_to_model = '../models/trt/model_static_max_batch_8_112_fp32.trt'\n",
    "trt_infer = TrtInfer(path_to_model=path_to_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "040e97b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trt_32 = get_pred_tensorrt(trt_infer, data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5582d3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['trt_32'] = y_trt_32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cec06fe",
   "metadata": {},
   "source": [
    "## Сравнение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0e249edb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>score</th>\n",
       "      <th>dist_cr_fiq</th>\n",
       "      <th>onnx_cpu</th>\n",
       "      <th>trt_16</th>\n",
       "      <th>trt_32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/dima/datasets/metro_dataset/1/336080.png</td>\n",
       "      <td>69.005943</td>\n",
       "      <td>71.733521</td>\n",
       "      <td>71.734154</td>\n",
       "      <td>71.875000</td>\n",
       "      <td>71.733887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/dima/datasets/metro_dataset/1/494830.png</td>\n",
       "      <td>77.437118</td>\n",
       "      <td>78.443466</td>\n",
       "      <td>78.443504</td>\n",
       "      <td>78.375000</td>\n",
       "      <td>78.444138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/dima/datasets/metro_dataset/1/398431.png</td>\n",
       "      <td>59.581174</td>\n",
       "      <td>54.542908</td>\n",
       "      <td>54.541039</td>\n",
       "      <td>54.531250</td>\n",
       "      <td>54.541477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/dima/datasets/metro_dataset/1/442370.png</td>\n",
       "      <td>41.054667</td>\n",
       "      <td>40.565643</td>\n",
       "      <td>40.567150</td>\n",
       "      <td>40.500000</td>\n",
       "      <td>40.565491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/dima/datasets/metro_dataset/1/124570.png</td>\n",
       "      <td>83.540477</td>\n",
       "      <td>80.295120</td>\n",
       "      <td>80.297531</td>\n",
       "      <td>80.250000</td>\n",
       "      <td>80.294365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41867</th>\n",
       "      <td>/home/dima/datasets/metro_dataset/0/407140.png</td>\n",
       "      <td>24.396170</td>\n",
       "      <td>27.609613</td>\n",
       "      <td>27.612160</td>\n",
       "      <td>27.390625</td>\n",
       "      <td>27.613712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41868</th>\n",
       "      <td>/home/dima/datasets/metro_dataset/0/83480.png</td>\n",
       "      <td>24.156331</td>\n",
       "      <td>35.607269</td>\n",
       "      <td>35.613720</td>\n",
       "      <td>35.312500</td>\n",
       "      <td>35.614758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41869</th>\n",
       "      <td>/home/dima/datasets/metro_dataset/0/373920.png</td>\n",
       "      <td>9.452589</td>\n",
       "      <td>14.309173</td>\n",
       "      <td>14.307244</td>\n",
       "      <td>14.320312</td>\n",
       "      <td>14.307057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41870</th>\n",
       "      <td>/home/dima/datasets/metro_dataset/0/522570.png</td>\n",
       "      <td>44.500259</td>\n",
       "      <td>36.960449</td>\n",
       "      <td>36.963650</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>36.964310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41871</th>\n",
       "      <td>/home/dima/datasets/metro_dataset/0/252940.png</td>\n",
       "      <td>21.443593</td>\n",
       "      <td>21.773411</td>\n",
       "      <td>21.772945</td>\n",
       "      <td>21.734375</td>\n",
       "      <td>21.774258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41872 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 path      score  dist_cr_fiq  \\\n",
       "0      /home/dima/datasets/metro_dataset/1/336080.png  69.005943    71.733521   \n",
       "1      /home/dima/datasets/metro_dataset/1/494830.png  77.437118    78.443466   \n",
       "2      /home/dima/datasets/metro_dataset/1/398431.png  59.581174    54.542908   \n",
       "3      /home/dima/datasets/metro_dataset/1/442370.png  41.054667    40.565643   \n",
       "4      /home/dima/datasets/metro_dataset/1/124570.png  83.540477    80.295120   \n",
       "...                                               ...        ...          ...   \n",
       "41867  /home/dima/datasets/metro_dataset/0/407140.png  24.396170    27.609613   \n",
       "41868   /home/dima/datasets/metro_dataset/0/83480.png  24.156331    35.607269   \n",
       "41869  /home/dima/datasets/metro_dataset/0/373920.png   9.452589    14.309173   \n",
       "41870  /home/dima/datasets/metro_dataset/0/522570.png  44.500259    36.960449   \n",
       "41871  /home/dima/datasets/metro_dataset/0/252940.png  21.443593    21.773411   \n",
       "\n",
       "        onnx_cpu     trt_16     trt_32  \n",
       "0      71.734154  71.875000  71.733887  \n",
       "1      78.443504  78.375000  78.444138  \n",
       "2      54.541039  54.531250  54.541477  \n",
       "3      40.567150  40.500000  40.565491  \n",
       "4      80.297531  80.250000  80.294365  \n",
       "...          ...        ...        ...  \n",
       "41867  27.612160  27.390625  27.613712  \n",
       "41868  35.613720  35.312500  35.614758  \n",
       "41869  14.307244  14.320312  14.307057  \n",
       "41870  36.963650  37.000000  36.964310  \n",
       "41871  21.772945  21.734375  21.774258  \n",
       "\n",
       "[41872 rows x 6 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780b38cc",
   "metadata": {},
   "source": [
    "### GT vs pytorch_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "233c2ebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.256822535186602"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse = criterion(torch.tensor(data['score'].values), torch.tensor(data['dist_cr_fiq'].values))\n",
    "mse.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a1015699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.9828983362735864"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae = metric(torch.tensor(data['score'].values), torch.tensor(data['dist_cr_fiq'].values))\n",
    "mae.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c0f422",
   "metadata": {},
   "source": [
    "### GT vs onnx_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0cb66c34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.256730431011688"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse = criterion(torch.tensor(data['score'].values), torch.tensor(data['onnx_cpu'].values))\n",
    "mse.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "26dabe19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.9828766223476695"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae = metric(torch.tensor(data['score'].values), torch.tensor(data['onnx_cpu'].values))\n",
    "mae.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3a8485",
   "metadata": {},
   "source": [
    "### GT vs pytorch_trt_16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "31e0f4e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.285615915977182"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse = criterion(torch.tensor(data['score'].values), torch.tensor(data['trt_16'].values))\n",
    "mse.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e4de2d77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.9858661173607968"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae = metric(torch.tensor(data['score'].values), torch.tensor(data['trt_16'].values))\n",
    "mae.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a7b6dd",
   "metadata": {},
   "source": [
    "### GT vs pytorch_trt_32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d2f2c17d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.256807634173805"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse = criterion(torch.tensor(data['score'].values), torch.tensor(data['trt_32'].values))\n",
    "mse.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9361ab10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.9828886746027137"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae = metric(torch.tensor(data['score'].values), torch.tensor(data['trt_32'].values))\n",
    "mae.item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
